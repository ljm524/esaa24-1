{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO2oD+5mEB2LbdKc8uQdO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljm524/esaa24-1/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtgC4TdrXM0m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, optimizers\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Embedding, BatchNormalization, SimpleRNN, LSTM\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical, plot_model, pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "random.seed(22)\n",
        "np.random.seed(22)\n",
        "tf.random.set_seed(22)\n",
        "원핫인코딩후traintestsplit(랜덤스테이트지정해야됨)\n",
        "minmax_scaler = preprocessing.MinMaxScaler()\n",
        "norm_fit = minmax_scaler.fit(X_train)\n",
        "X_train_norm = norm_fit.transform(X_train)\n",
        "X_test_norm = norm_fit.transform(X_test)\n",
        "standard_scaler = preprocessing.StandardScaler()\n",
        "stan_fit = standard_scaler.fit(X_train)\n",
        "X_train_stan = stan_fit.transform(X_train)\n",
        "X_test_stan = stan_fit.transform(X_test)\n",
        "model = keras.Sequential([\n",
        "    # input layer\n",
        "    Dense(node개수,activation='relu',input_dim아니면shape=설명변수개수(10,),kernel_initializer=\"he_normal\":안해도됨),\n",
        "    BatchNormalization(),:해도되고안해도됨\n",
        "    Dropout(0.2),:해도되고안해도됨20%날려\n",
        "    # hidden layer\n",
        "    Dense(node개수,activation='relu'),\n",
        "    BatchNormalization(),:근데하려면다해야하는듯\n",
        "    Dropout(0.2),:이것도하려면다해야하는듯안해도될듯..\n",
        "    # output layer\n",
        "    Dense(class개수,activation='linear아니면softmax')])\n",
        "model.compile(optimizer='rmsprop'아니면tf.keras.optimizers.Adam아니면RMSprop(learning_rate=0.06),loss='categorical_crossentropy 아니면 binary_crossentropy 아니면 mse',metrics=['acurracy 아니면 mse'])\n",
        "early_stop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\n",
        "history = model.fit(callbacks=[early_stop],X_train_norm,y_train,batch_size=32,epochs=100,validation_split=0.2아니면validation_data=(x_test,y_test),verbose=1:상세하게출력,0:출력x,2:대략적출)\n",
        "res = model.evaluate(X_test,y_test,verbose=1)\n",
        "res[1]:mse아님accuracy res[0]:loss\n",
        "plt.scatter(y_test,model.predict(X_test))\n",
        "plt.plot(y_test,y_test)\n",
        "from sklearn.metrics import r2_score, confusion_matrix\n",
        "r2_score(y_test,model.predict(X_test))\n",
        "pred=(model.predict(X_test)>0.5)꼭0.5로설정안해도될듯걍가도될\n",
        "y_pred=[] // y_test_c = []\n",
        "for i in range(0,len(y_test)):\n",
        "    k = np.argmax(pred[i]아니면y_test.iloc[i,:])\n",
        "    y_pred // y_test_c.append(k)\n",
        "confusion_matrix(y_test_c,y_pred)이제cnn\n",
        "(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()\n",
        "input_shape=(row개수,col개수,rgb면3)\n",
        "x_train//test = x_train//test.reshape(x_train//test.shape[0],row수,col수,rgb).astype('float32')\n",
        "x_train//test /= 255.0\n",
        "y_train//test = to_categorical(y_train//test,num_classes)\n",
        "tf.keras.Sequential([tf.keras.layers.Conv2D(32,kernal_initializer=\"he_uniform\",padding=\"same\",filter_size=3,strides=1,input_shape=input_shape,activation='relu'),\n",
        "                      tf.keras.layers.MaxPooling2D(pool_size=2,strides=2),\n",
        "                     tf.keras.layers.Conv2D(64,kernal_size=(2,2),activation='relu',kernal_initializer=\"he_uniform\",padding=\"same\"),\n",
        "                     tf.keras.layers.Maxpooling2D(pool_size=(2,2),strides=2)\n",
        "                      tf.keras.layers.Flatten(),\n",
        "                      tf.keras.layers.Dense(1024,activation='relu'),\n",
        "                     tf.keras.layers.Dropout(0.5)\n",
        "                      tf.keras.layers.Dense(10,activation='softmax')])\n",
        "compile fit 위에랑똑같\n",
        "confusion_matrix(np.argmax(y_test,axis=1),np.argmax(ypred,axis=1))\n",
        "#VGG-16//Inception\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip,'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "base_dir='/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "validation_dir = os.path.join(base_dir,'validation')\n",
        "train_cats_dir=os.path.join(train_dir,'cats')\n",
        "train_dogs_dir=os.path.join(train_dir,'dogs')\n",
        "validation_cats_dir=os.path.join(validation_dir,'cats')\n",
        "validation_dogs_dir=os.path.join(validation_dir,'dogs')\n",
        "train_datagen=ImageDataGenerator(rescale=1./255.,rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
        "test_datagen+ImageDataGenerator(rescale=1.0/255.)\n",
        "train_generator=train_datagen.flow_from_directory(train_dir,batch_size=20,class_mode='binary',target_size=(224//150,224//150))\n",
        "validation_generator=test_datagen.fow_from_directory(validation_dir,batch_size=20,class_mode='binary',target_size=(224//150,224//150))\n",
        "from tensorflow.keras.applications.vgg16//inception_v3 import VGG16//InceptionV3\n",
        "base_model=VGG16(input_shape=(224,224,3),include_top=False,weights='imagenet')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "x=layers.Flatten()(base_model.output)\n",
        "x=layers.Dense(512//1024,activation='relu')(x)\n",
        "x=layers.Dropout(0.5//0.2)(x)\n",
        "x=layers.Dense(1,activation='sigmoid')(x)\n",
        "model=tf.keras.models.Model(base_model.input,x)\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),loss='binary_crossentropy',metric=['acc'])\n",
        "vgghist=model.fit(train_generator,validation_data=validation_generator,steps_per_epoch=100,epochs=10)이제rnn\n",
        "Sequential(SimpleRNN(units=128,activation='tanh',input_shape=(20,1)),Dense(1))\n",
        "raw_df['3MA']=raw_df['Close'].rolling(window=3).mean()\n",
        "raw_df['5MA']=raw_df['Close'].rolling(window=5).mean()\n",
        "raw_df['3MA_new']=np.NaN\n",
        "raw_df['5MA_new']=np.NaN\n",
        "for i in range(len(raw_df)-1):\n",
        "    raw_df.iloc[i+1,9]=raw_df.iloc[i,7]\n",
        "    raw_df.iloc[i+1,10]=raw_df.iloc[i,8]\n",
        "df2=raw_df.copy()\n",
        "raw_df.loc[raw_df['Volume']==0]\n",
        "raw_df['Volume']=raw_df['Volume'].replace(0,np.nan)\n",
        "raw_df=raw_df.dropna()\n",
        "scaler=MinMaxScaler()\n",
        "scale_cols=['Open','High','Low','Close','Adj Close','3MA','5MA','Volume','3MA_new','5MA_new']\n",
        "scaled_df = scaler.fit_transform(raw_df[scale_cols])\n",
        "scaled_df = pd.DataFrame(scaled_df,columns=scale_cols)\n",
        "def make_sequence_dataset(feature,label,window_size):\n",
        "    feature_list=[]\n",
        "    label_list=[]\n",
        "    for i in range(len(feature)-window_size):\n",
        "        feature_list.append(feature[i:i+window_size])\n",
        "        label_list.append(label[i+window_size])\n",
        "    return np.array(feature_list), np.array(label_list)\n",
        "feature_cols=['3MA_new','5MA_new']\n",
        "label_cols=['Close']\n",
        "feature_df=pd.DataFrame(scaled_df,columns=feature_cols)\n",
        "label_df=pd.DataFrame(scaled_df,columns=label_cols)\n",
        "feature_np=feature_df.to_numpy()\n",
        "label_np=label_df.to_numpy()\n",
        "window_size=40\n",
        "X,Y=make_sequence_dataset(feature_np,label_np,window_size)\n",
        "split=int(len(X)*0.95)\n",
        "x_train=X[0:split]\n",
        "y_train=Y[0:split]\n",
        "x_test=X[split:]\n",
        "y_test=Y[split:]\n",
        "model=Sequential([LSTM(128,activation='tanh'),input_shape=x_train[0].shape],Dense(1,activation='linear'))\n",
        "compile fit 똑같이\n",
        "df2['R_t']=np.NaN\n",
        "for i in range(len(df2)-1):\n",
        "    y0 = df2.iloc[i,4]\n",
        "    y1 = df2.iloc[i+1,4]\n",
        "    df2.iloc[i+1,11]=(y1-y0)/y0\n",
        "df2['Volume']=df2['Volume'].replace(0,np.nan)\n",
        "df2=df2.dropna()\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler()\n",
        "scale_cols=['3MA_new','5MA_new','R_t']\n",
        "scaled_df2=scaler.fit_transform(df2[scale_cols])\n",
        "scaled_df2=pd.DataFrame(scaled_df2,columns=scale_cols)\n",
        "feature_cols=['3MA_new','5MA_new']\n",
        "label_cols=['R_t']\n",
        "feature_df=pd.DataFrame(scaled_df2,columns=feature_cols)\n",
        "label_df=pd.DataFrame(df2,columns=label_cols)\n",
        "위에랑똑같\n",
        "model_gru = Sequential([GRU(256,activation='tanh',input_shape=x_train[0].shape),\n",
        "                        Dense(1,activation='linear')])\n",
        "compile ift 똑같이\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "samples=['The cat sat on the mat.','The dog ate my homework.']\n",
        "tokenizer=Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences=tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results=tokenizer.texts_to_matrix(samples,mode='binary')\n",
        "word_index=tokenizer.word_index\n",
        "print(f'Found {len(word_index)} unique tokens.')\n",
        "import os\n",
        "imdb_dir = '/content/drive/MyDrive/이름'\n",
        "train_dir=os.path.join(imdb_dir,'train')\n",
        "labels=[]\n",
        "tests=[]\n",
        "for label_type in ['neg','pos']:\n",
        "    dir_name = os.path.join(train_dir,label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:]=='.txt':\n",
        "            f=open(os.path.join(dir_name,fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "maxlen=100\n",
        "max_words = 10000\n",
        "tokenizer=Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences=tokenizer.texts_to_sequences(texts)\n",
        "word_index=tokenizer.word_index\n",
        "data=pad_sequences(sequences,maxlen=maxlen)\n",
        "labels=np.asarray(labels)\n",
        "indices=np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data=data[indices]\n",
        "labels=labels[indices]\n",
        "training_samples=20000\n",
        "validation_samples=5000\n",
        "x_train=data[:training_samples]\n",
        "y_train=labels[:training_samples]\n",
        "x_val=data[training_samples:(training_samples+validation_samples)]\n",
        "y_val=label[training_samples:(training_samples+validation_samples)]\n",
        "model=Sequential([Embedding(max_words,100,input_length=maxlen),\n",
        "            Flatten(), Dense(32,activation='relu'),\n",
        "            Dense(1,activation='sigmoid')])\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "history 똑같이\n",
        "test_dir=os.path.oin(imdb_dir,'test')\n",
        "위랑똑\n",
        "for label_type in ['neg','pos']:\n",
        "    dir_name = os.path.join(test_dir,label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:]=='.txt':\n",
        "            f=open(os.path.join(dir_name,fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "sequences=tokenizer.texts_to_sequences(texts)\n",
        "x_test=pad_sequences(sequences,maxlen=maxlen)\n",
        "y_test=np.asarray(labels)\n",
        "model.evaluate(x_test,y_test)"
      ]
    }
  ]
}